{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3111ebc4",
   "metadata": {},
   "source": [
    "# 微调在ImageNet上预训练的卷积神经网络实现鸟类识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a10cd",
   "metadata": {},
   "source": [
    "## 模型介绍 : \n",
    "## 1. 修改ResNet-18架构用于鸟类识别，通过将其输出层大小设置为200以适应数据集中的类别数量，其余层使用在ImageNet上预训练得到的网络参数进行初始化。\n",
    "## 2. 在[CUB-200-2011]( https://data.caltech.edu/records/65de6-vp158)数据集（训练集：80%， 验证集：20%）上从零开始训练新的输出层，并对其余参数使用较小的学习率进行微调\n",
    "## 3. 通过调整不同的超参数组合（LR:[0.01, 0.001]）(epoch:[10, 15, 20]) (batch_size:[16, 32]),观察在验证集上面的效果，最后找到最好的参数组合\n",
    "## 4.使用最好的参数组合与从随机初始化的网络参数开始训练得到的参数组合进行对比，观察最后的实验效果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e592b4c",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89354c54",
   "metadata": {},
   "source": [
    "# 模型权重链接： https://pan.baidu.com/s/1a3BqgSVdEmhYPXWSuXyPvQ?pwd=xjhz 提取码: xjhz --来自百度网盘超级会员v6的分享\n",
    "# 数据集相关链接：https://data.caltech.edu/records/65de6-vp158\n",
    "# github链接：https://github.com/liuyhoong/-ImageNet-.git \n",
    "# 相关代码： git clone https://github.com/liuyhoong/-ImageNet-.git "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e734d",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1059e9",
   "metadata": {},
   "source": [
    "# 先导入必须的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "704b9848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a4ae98",
   "metadata": {},
   "source": [
    "#  选择预训练模型ResNet-18并调整输出层大小为200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40c3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\yolov8\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\yolov8\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\13277/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# 选择预训练模型ResNet-18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# 修改最后一层，全连接层的输入大小保持不变，输出大小设置为200\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d373f883",
   "metadata": {},
   "source": [
    "# 读取数据集并划分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5998ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 11788\n",
      "Training images: 9414\n",
      "Validation images: 2374\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "\n",
    "data_dir = 'D:/FDU/graduate/研一下/神网/期中作业/CUB_200_2011/CUB_200_2011/images'\n",
    "\n",
    "# 定义数据转换\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 加载整个数据集\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "\n",
    "# 获取类别索引\n",
    "class_indices = {cls: [] for cls in full_dataset.classes}\n",
    "\n",
    "# 按类别收集图像索引\n",
    "for idx, (path, class_idx) in enumerate(full_dataset.imgs):\n",
    "    class_indices[full_dataset.classes[class_idx]].append(idx)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for cls, indices in class_indices.items():\n",
    "    random.shuffle(indices)\n",
    "    split = int(0.8 * len(indices))  # 80% 训练，20% 验证\n",
    "    train_indices.extend(indices[:split])\n",
    "    val_indices.extend(indices[split:])\n",
    "\n",
    "# 创建训练集和验证集\n",
    "train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "\n",
    "# 应用不同的transform\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "# 创建DataLoader\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 输出一些信息以确认\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb66ac",
   "metadata": {},
   "source": [
    "# 找到最好的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1382c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: 0.01, Epoch: 0/9, Batch Size: 16, train Loss: 3.1294 Acc: 0.3187\n",
      "LR: 0.01, Epoch: 0/9, Batch Size: 16, val Loss: 1.6145 Acc: 0.5826\n",
      "LR: 0.01, Epoch: 1/9, Batch Size: 16, train Loss: 1.1407 Acc: 0.7167\n",
      "LR: 0.01, Epoch: 1/9, Batch Size: 16, val Loss: 1.1463 Acc: 0.6967\n",
      "LR: 0.01, Epoch: 2/9, Batch Size: 16, train Loss: 0.6435 Acc: 0.8421\n",
      "LR: 0.01, Epoch: 2/9, Batch Size: 16, val Loss: 0.9918 Acc: 0.7279\n",
      "LR: 0.01, Epoch: 3/9, Batch Size: 16, train Loss: 0.3727 Acc: 0.9220\n",
      "LR: 0.01, Epoch: 3/9, Batch Size: 16, val Loss: 0.9730 Acc: 0.7279\n",
      "LR: 0.01, Epoch: 4/9, Batch Size: 16, train Loss: 0.2195 Acc: 0.9638\n",
      "LR: 0.01, Epoch: 4/9, Batch Size: 16, val Loss: 0.9416 Acc: 0.7397\n",
      "LR: 0.01, Epoch: 5/9, Batch Size: 16, train Loss: 0.1277 Acc: 0.9869\n",
      "LR: 0.01, Epoch: 5/9, Batch Size: 16, val Loss: 0.9147 Acc: 0.7447\n",
      "LR: 0.01, Epoch: 6/9, Batch Size: 16, train Loss: 0.0816 Acc: 0.9954\n",
      "LR: 0.01, Epoch: 6/9, Batch Size: 16, val Loss: 0.9089 Acc: 0.7511\n",
      "LR: 0.01, Epoch: 7/9, Batch Size: 16, train Loss: 0.0573 Acc: 0.9987\n",
      "LR: 0.01, Epoch: 7/9, Batch Size: 16, val Loss: 0.8989 Acc: 0.7595\n",
      "LR: 0.01, Epoch: 8/9, Batch Size: 16, train Loss: 0.0480 Acc: 0.9993\n",
      "LR: 0.01, Epoch: 8/9, Batch Size: 16, val Loss: 0.9021 Acc: 0.7624\n",
      "LR: 0.01, Epoch: 9/9, Batch Size: 16, train Loss: 0.0355 Acc: 0.9996\n",
      "LR: 0.01, Epoch: 9/9, Batch Size: 16, val Loss: 0.8925 Acc: 0.7654\n",
      "LR: 0.01, Epoch: 0/9, Batch Size: 32, train Loss: 3.1168 Acc: 0.3231\n",
      "LR: 0.01, Epoch: 0/9, Batch Size: 32, val Loss: 1.5784 Acc: 0.5956\n",
      "LR: 0.01, Epoch: 1/9, Batch Size: 32, train Loss: 1.1398 Acc: 0.7140\n",
      "LR: 0.01, Epoch: 1/9, Batch Size: 32, val Loss: 1.1397 Acc: 0.6925\n",
      "LR: 0.01, Epoch: 2/9, Batch Size: 32, train Loss: 0.6361 Acc: 0.8415\n",
      "LR: 0.01, Epoch: 2/9, Batch Size: 32, val Loss: 0.9979 Acc: 0.7207\n",
      "LR: 0.01, Epoch: 3/9, Batch Size: 32, train Loss: 0.3733 Acc: 0.9185\n",
      "LR: 0.01, Epoch: 3/9, Batch Size: 32, val Loss: 0.9526 Acc: 0.7460\n",
      "LR: 0.01, Epoch: 4/9, Batch Size: 32, train Loss: 0.2168 Acc: 0.9666\n",
      "LR: 0.01, Epoch: 4/9, Batch Size: 32, val Loss: 0.9118 Acc: 0.7536\n",
      "LR: 0.01, Epoch: 5/9, Batch Size: 32, train Loss: 0.1280 Acc: 0.9890\n",
      "LR: 0.01, Epoch: 5/9, Batch Size: 32, val Loss: 0.8883 Acc: 0.7654\n",
      "LR: 0.01, Epoch: 6/9, Batch Size: 32, train Loss: 0.0814 Acc: 0.9962\n",
      "LR: 0.01, Epoch: 6/9, Batch Size: 32, val Loss: 0.8902 Acc: 0.7645\n",
      "LR: 0.01, Epoch: 7/9, Batch Size: 32, train Loss: 0.0566 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 7/9, Batch Size: 32, val Loss: 0.8902 Acc: 0.7633\n",
      "LR: 0.01, Epoch: 8/9, Batch Size: 32, train Loss: 0.0482 Acc: 0.9989\n",
      "LR: 0.01, Epoch: 8/9, Batch Size: 32, val Loss: 0.8836 Acc: 0.7679\n",
      "LR: 0.01, Epoch: 9/9, Batch Size: 32, train Loss: 0.0346 Acc: 0.9999\n",
      "LR: 0.01, Epoch: 9/9, Batch Size: 32, val Loss: 0.8833 Acc: 0.7612\n",
      "LR: 0.01, Epoch: 0/14, Batch Size: 16, train Loss: 3.1019 Acc: 0.3204\n",
      "LR: 0.01, Epoch: 0/14, Batch Size: 16, val Loss: 1.5619 Acc: 0.5977\n",
      "LR: 0.01, Epoch: 1/14, Batch Size: 16, train Loss: 1.1246 Acc: 0.7151\n",
      "LR: 0.01, Epoch: 1/14, Batch Size: 16, val Loss: 1.1675 Acc: 0.6849\n",
      "LR: 0.01, Epoch: 2/14, Batch Size: 16, train Loss: 0.6482 Acc: 0.8407\n",
      "LR: 0.01, Epoch: 2/14, Batch Size: 16, val Loss: 1.0121 Acc: 0.7106\n",
      "LR: 0.01, Epoch: 3/14, Batch Size: 16, train Loss: 0.3794 Acc: 0.9217\n",
      "LR: 0.01, Epoch: 3/14, Batch Size: 16, val Loss: 0.9419 Acc: 0.7388\n",
      "LR: 0.01, Epoch: 4/14, Batch Size: 16, train Loss: 0.2150 Acc: 0.9674\n",
      "LR: 0.01, Epoch: 4/14, Batch Size: 16, val Loss: 0.9174 Acc: 0.7506\n",
      "LR: 0.01, Epoch: 5/14, Batch Size: 16, train Loss: 0.1288 Acc: 0.9880\n",
      "LR: 0.01, Epoch: 5/14, Batch Size: 16, val Loss: 0.9145 Acc: 0.7515\n",
      "LR: 0.01, Epoch: 6/14, Batch Size: 16, train Loss: 0.0804 Acc: 0.9969\n",
      "LR: 0.01, Epoch: 6/14, Batch Size: 16, val Loss: 0.8898 Acc: 0.7502\n",
      "LR: 0.01, Epoch: 7/14, Batch Size: 16, train Loss: 0.0581 Acc: 0.9982\n",
      "LR: 0.01, Epoch: 7/14, Batch Size: 16, val Loss: 0.8943 Acc: 0.7515\n",
      "LR: 0.01, Epoch: 8/14, Batch Size: 16, train Loss: 0.0429 Acc: 0.9995\n",
      "LR: 0.01, Epoch: 8/14, Batch Size: 16, val Loss: 0.8835 Acc: 0.7658\n",
      "LR: 0.01, Epoch: 9/14, Batch Size: 16, train Loss: 0.0353 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 9/14, Batch Size: 16, val Loss: 0.8841 Acc: 0.7628\n",
      "LR: 0.01, Epoch: 10/14, Batch Size: 16, train Loss: 0.0303 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 10/14, Batch Size: 16, val Loss: 0.8923 Acc: 0.7607\n",
      "LR: 0.01, Epoch: 11/14, Batch Size: 16, train Loss: 0.0259 Acc: 0.9999\n",
      "LR: 0.01, Epoch: 11/14, Batch Size: 16, val Loss: 0.8884 Acc: 0.7666\n",
      "LR: 0.01, Epoch: 12/14, Batch Size: 16, train Loss: 0.0248 Acc: 0.9996\n",
      "LR: 0.01, Epoch: 12/14, Batch Size: 16, val Loss: 0.8857 Acc: 0.7692\n",
      "LR: 0.01, Epoch: 13/14, Batch Size: 16, train Loss: 0.0216 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 13/14, Batch Size: 16, val Loss: 0.8959 Acc: 0.7641\n",
      "LR: 0.01, Epoch: 14/14, Batch Size: 16, train Loss: 0.0210 Acc: 0.9996\n",
      "LR: 0.01, Epoch: 14/14, Batch Size: 16, val Loss: 0.8957 Acc: 0.7666\n",
      "LR: 0.01, Epoch: 0/14, Batch Size: 32, train Loss: 3.1264 Acc: 0.3181\n",
      "LR: 0.01, Epoch: 0/14, Batch Size: 32, val Loss: 1.6172 Acc: 0.5788\n",
      "LR: 0.01, Epoch: 1/14, Batch Size: 32, train Loss: 1.1289 Acc: 0.7145\n",
      "LR: 0.01, Epoch: 1/14, Batch Size: 32, val Loss: 1.1285 Acc: 0.6967\n",
      "LR: 0.01, Epoch: 2/14, Batch Size: 32, train Loss: 0.6455 Acc: 0.8384\n",
      "LR: 0.01, Epoch: 2/14, Batch Size: 32, val Loss: 1.0327 Acc: 0.7043\n",
      "LR: 0.01, Epoch: 3/14, Batch Size: 32, train Loss: 0.3720 Acc: 0.9242\n",
      "LR: 0.01, Epoch: 3/14, Batch Size: 32, val Loss: 0.9734 Acc: 0.7350\n",
      "LR: 0.01, Epoch: 4/14, Batch Size: 32, train Loss: 0.2196 Acc: 0.9642\n",
      "LR: 0.01, Epoch: 4/14, Batch Size: 32, val Loss: 0.9053 Acc: 0.7540\n",
      "LR: 0.01, Epoch: 5/14, Batch Size: 32, train Loss: 0.1217 Acc: 0.9914\n",
      "LR: 0.01, Epoch: 5/14, Batch Size: 32, val Loss: 0.9047 Acc: 0.7523\n",
      "LR: 0.01, Epoch: 6/14, Batch Size: 32, train Loss: 0.0787 Acc: 0.9973\n",
      "LR: 0.01, Epoch: 6/14, Batch Size: 32, val Loss: 0.9056 Acc: 0.7494\n",
      "LR: 0.01, Epoch: 7/14, Batch Size: 32, train Loss: 0.0601 Acc: 0.9978\n",
      "LR: 0.01, Epoch: 7/14, Batch Size: 32, val Loss: 0.8975 Acc: 0.7570\n",
      "LR: 0.01, Epoch: 8/14, Batch Size: 32, train Loss: 0.0436 Acc: 0.9996\n",
      "LR: 0.01, Epoch: 8/14, Batch Size: 32, val Loss: 0.8918 Acc: 0.7645\n",
      "LR: 0.01, Epoch: 9/14, Batch Size: 32, train Loss: 0.0348 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 9/14, Batch Size: 32, val Loss: 0.8862 Acc: 0.7624\n",
      "LR: 0.01, Epoch: 10/14, Batch Size: 32, train Loss: 0.0288 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 10/14, Batch Size: 32, val Loss: 0.8902 Acc: 0.7616\n",
      "LR: 0.01, Epoch: 11/14, Batch Size: 32, train Loss: 0.0248 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 11/14, Batch Size: 32, val Loss: 0.8968 Acc: 0.7603\n",
      "LR: 0.01, Epoch: 12/14, Batch Size: 32, train Loss: 0.0222 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 12/14, Batch Size: 32, val Loss: 0.8903 Acc: 0.7658\n",
      "LR: 0.01, Epoch: 13/14, Batch Size: 32, train Loss: 0.0206 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 13/14, Batch Size: 32, val Loss: 0.8953 Acc: 0.7620\n",
      "LR: 0.01, Epoch: 14/14, Batch Size: 32, train Loss: 0.0182 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 14/14, Batch Size: 32, val Loss: 0.8934 Acc: 0.7628\n",
      "LR: 0.01, Epoch: 0/19, Batch Size: 16, train Loss: 3.1051 Acc: 0.3237\n",
      "LR: 0.01, Epoch: 0/19, Batch Size: 16, val Loss: 1.5818 Acc: 0.5864\n",
      "LR: 0.01, Epoch: 1/19, Batch Size: 16, train Loss: 1.1287 Acc: 0.7145\n",
      "LR: 0.01, Epoch: 1/19, Batch Size: 16, val Loss: 1.1238 Acc: 0.7013\n",
      "LR: 0.01, Epoch: 2/19, Batch Size: 16, train Loss: 0.6411 Acc: 0.8443\n",
      "LR: 0.01, Epoch: 2/19, Batch Size: 16, val Loss: 1.0132 Acc: 0.7237\n",
      "LR: 0.01, Epoch: 3/19, Batch Size: 16, train Loss: 0.3731 Acc: 0.9240\n",
      "LR: 0.01, Epoch: 3/19, Batch Size: 16, val Loss: 0.9506 Acc: 0.7359\n",
      "LR: 0.01, Epoch: 4/19, Batch Size: 16, train Loss: 0.2248 Acc: 0.9635\n",
      "LR: 0.01, Epoch: 4/19, Batch Size: 16, val Loss: 0.9246 Acc: 0.7544\n",
      "LR: 0.01, Epoch: 5/19, Batch Size: 16, train Loss: 0.1263 Acc: 0.9891\n",
      "LR: 0.01, Epoch: 5/19, Batch Size: 16, val Loss: 0.8972 Acc: 0.7595\n",
      "LR: 0.01, Epoch: 6/19, Batch Size: 16, train Loss: 0.0810 Acc: 0.9972\n",
      "LR: 0.01, Epoch: 6/19, Batch Size: 16, val Loss: 0.8949 Acc: 0.7582\n",
      "LR: 0.01, Epoch: 7/19, Batch Size: 16, train Loss: 0.0556 Acc: 0.9993\n",
      "LR: 0.01, Epoch: 7/19, Batch Size: 16, val Loss: 0.8889 Acc: 0.7591\n",
      "LR: 0.01, Epoch: 8/19, Batch Size: 16, train Loss: 0.0461 Acc: 0.9993\n",
      "LR: 0.01, Epoch: 8/19, Batch Size: 16, val Loss: 0.8770 Acc: 0.7679\n",
      "LR: 0.01, Epoch: 9/19, Batch Size: 16, train Loss: 0.0351 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 9/19, Batch Size: 16, val Loss: 0.8924 Acc: 0.7548\n",
      "LR: 0.01, Epoch: 10/19, Batch Size: 16, train Loss: 0.0307 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 10/19, Batch Size: 16, val Loss: 0.8924 Acc: 0.7641\n",
      "LR: 0.01, Epoch: 11/19, Batch Size: 16, train Loss: 0.0261 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 11/19, Batch Size: 16, val Loss: 0.8846 Acc: 0.7650\n",
      "LR: 0.01, Epoch: 12/19, Batch Size: 16, train Loss: 0.0233 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 12/19, Batch Size: 16, val Loss: 0.8906 Acc: 0.7633\n",
      "LR: 0.01, Epoch: 13/19, Batch Size: 16, train Loss: 0.0205 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 13/19, Batch Size: 16, val Loss: 0.8873 Acc: 0.7666\n",
      "LR: 0.01, Epoch: 14/19, Batch Size: 16, train Loss: 0.0199 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 14/19, Batch Size: 16, val Loss: 0.8854 Acc: 0.7628\n",
      "LR: 0.01, Epoch: 15/19, Batch Size: 16, train Loss: 0.0168 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 15/19, Batch Size: 16, val Loss: 0.8895 Acc: 0.7658\n",
      "LR: 0.01, Epoch: 16/19, Batch Size: 16, train Loss: 0.0159 Acc: 0.9999\n",
      "LR: 0.01, Epoch: 16/19, Batch Size: 16, val Loss: 0.8904 Acc: 0.7713\n",
      "LR: 0.01, Epoch: 17/19, Batch Size: 16, train Loss: 0.0155 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 17/19, Batch Size: 16, val Loss: 0.8889 Acc: 0.7683\n",
      "LR: 0.01, Epoch: 18/19, Batch Size: 16, train Loss: 0.0140 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 18/19, Batch Size: 16, val Loss: 0.8927 Acc: 0.7624\n",
      "LR: 0.01, Epoch: 19/19, Batch Size: 16, train Loss: 0.0131 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 19/19, Batch Size: 16, val Loss: 0.8965 Acc: 0.7654\n",
      "LR: 0.01, Epoch: 0/19, Batch Size: 32, train Loss: 3.0946 Acc: 0.3289\n",
      "LR: 0.01, Epoch: 0/19, Batch Size: 32, val Loss: 1.5397 Acc: 0.5931\n",
      "LR: 0.01, Epoch: 1/19, Batch Size: 32, train Loss: 1.1411 Acc: 0.7141\n",
      "LR: 0.01, Epoch: 1/19, Batch Size: 32, val Loss: 1.1243 Acc: 0.6967\n",
      "LR: 0.01, Epoch: 2/19, Batch Size: 32, train Loss: 0.6353 Acc: 0.8481\n",
      "LR: 0.01, Epoch: 2/19, Batch Size: 32, val Loss: 1.0285 Acc: 0.7081\n",
      "LR: 0.01, Epoch: 3/19, Batch Size: 32, train Loss: 0.3727 Acc: 0.9195\n",
      "LR: 0.01, Epoch: 3/19, Batch Size: 32, val Loss: 0.9636 Acc: 0.7325\n",
      "LR: 0.01, Epoch: 4/19, Batch Size: 32, train Loss: 0.2225 Acc: 0.9638\n",
      "LR: 0.01, Epoch: 4/19, Batch Size: 32, val Loss: 0.9096 Acc: 0.7544\n",
      "LR: 0.01, Epoch: 5/19, Batch Size: 32, train Loss: 0.1301 Acc: 0.9870\n",
      "LR: 0.01, Epoch: 5/19, Batch Size: 32, val Loss: 0.8857 Acc: 0.7586\n",
      "LR: 0.01, Epoch: 6/19, Batch Size: 32, train Loss: 0.0851 Acc: 0.9960\n",
      "LR: 0.01, Epoch: 6/19, Batch Size: 32, val Loss: 0.8889 Acc: 0.7616\n",
      "LR: 0.01, Epoch: 7/19, Batch Size: 32, train Loss: 0.0597 Acc: 0.9989\n",
      "LR: 0.01, Epoch: 7/19, Batch Size: 32, val Loss: 0.8739 Acc: 0.7675\n",
      "LR: 0.01, Epoch: 8/19, Batch Size: 32, train Loss: 0.0435 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 8/19, Batch Size: 32, val Loss: 0.8763 Acc: 0.7709\n",
      "LR: 0.01, Epoch: 9/19, Batch Size: 32, train Loss: 0.0343 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 9/19, Batch Size: 32, val Loss: 0.8784 Acc: 0.7620\n",
      "LR: 0.01, Epoch: 10/19, Batch Size: 32, train Loss: 0.0326 Acc: 0.9990\n",
      "LR: 0.01, Epoch: 10/19, Batch Size: 32, val Loss: 0.8771 Acc: 0.7650\n",
      "LR: 0.01, Epoch: 11/19, Batch Size: 32, train Loss: 0.0278 Acc: 0.9996\n",
      "LR: 0.01, Epoch: 11/19, Batch Size: 32, val Loss: 0.8802 Acc: 0.7704\n",
      "LR: 0.01, Epoch: 12/19, Batch Size: 32, train Loss: 0.0226 Acc: 0.9999\n",
      "LR: 0.01, Epoch: 12/19, Batch Size: 32, val Loss: 0.8769 Acc: 0.7687\n",
      "LR: 0.01, Epoch: 13/19, Batch Size: 32, train Loss: 0.0206 Acc: 0.9998\n",
      "LR: 0.01, Epoch: 13/19, Batch Size: 32, val Loss: 0.8789 Acc: 0.7709\n",
      "LR: 0.01, Epoch: 14/19, Batch Size: 32, train Loss: 0.0200 Acc: 0.9997\n",
      "LR: 0.01, Epoch: 14/19, Batch Size: 32, val Loss: 0.8802 Acc: 0.7704\n",
      "LR: 0.01, Epoch: 15/19, Batch Size: 32, train Loss: 0.0169 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 15/19, Batch Size: 32, val Loss: 0.8811 Acc: 0.7751\n",
      "LR: 0.01, Epoch: 16/19, Batch Size: 32, train Loss: 0.0158 Acc: 0.9999\n",
      "LR: 0.01, Epoch: 16/19, Batch Size: 32, val Loss: 0.8820 Acc: 0.7725\n",
      "LR: 0.01, Epoch: 17/19, Batch Size: 32, train Loss: 0.0150 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 17/19, Batch Size: 32, val Loss: 0.8923 Acc: 0.7696\n",
      "LR: 0.01, Epoch: 18/19, Batch Size: 32, train Loss: 0.0135 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 18/19, Batch Size: 32, val Loss: 0.8972 Acc: 0.7658\n",
      "LR: 0.01, Epoch: 19/19, Batch Size: 32, train Loss: 0.0120 Acc: 1.0000\n",
      "LR: 0.01, Epoch: 19/19, Batch Size: 32, val Loss: 0.8916 Acc: 0.7650\n",
      "LR: 0.001, Epoch: 0/9, Batch Size: 16, train Loss: 5.0061 Acc: 0.0527\n",
      "LR: 0.001, Epoch: 0/9, Batch Size: 16, val Loss: 4.5031 Acc: 0.1559\n",
      "LR: 0.001, Epoch: 1/9, Batch Size: 16, train Loss: 4.0992 Acc: 0.2503\n",
      "LR: 0.001, Epoch: 1/9, Batch Size: 16, val Loss: 3.6524 Acc: 0.3138\n",
      "LR: 0.001, Epoch: 2/9, Batch Size: 16, train Loss: 3.3010 Acc: 0.4307\n",
      "LR: 0.001, Epoch: 2/9, Batch Size: 16, val Loss: 2.9684 Acc: 0.4596\n",
      "LR: 0.001, Epoch: 3/9, Batch Size: 16, train Loss: 2.6859 Acc: 0.5459\n",
      "LR: 0.001, Epoch: 3/9, Batch Size: 16, val Loss: 2.4723 Acc: 0.5413\n",
      "LR: 0.001, Epoch: 4/9, Batch Size: 16, train Loss: 2.2441 Acc: 0.6221\n",
      "LR: 0.001, Epoch: 4/9, Batch Size: 16, val Loss: 2.1745 Acc: 0.5977\n",
      "LR: 0.001, Epoch: 5/9, Batch Size: 16, train Loss: 1.9266 Acc: 0.6746\n",
      "LR: 0.001, Epoch: 5/9, Batch Size: 16, val Loss: 1.9168 Acc: 0.6154\n",
      "LR: 0.001, Epoch: 6/9, Batch Size: 16, train Loss: 1.6814 Acc: 0.7126\n",
      "LR: 0.001, Epoch: 6/9, Batch Size: 16, val Loss: 1.7244 Acc: 0.6470\n",
      "LR: 0.001, Epoch: 7/9, Batch Size: 16, train Loss: 1.4932 Acc: 0.7396\n",
      "LR: 0.001, Epoch: 7/9, Batch Size: 16, val Loss: 1.5947 Acc: 0.6634\n",
      "LR: 0.001, Epoch: 8/9, Batch Size: 16, train Loss: 1.3462 Acc: 0.7640\n",
      "LR: 0.001, Epoch: 8/9, Batch Size: 16, val Loss: 1.4861 Acc: 0.6786\n",
      "LR: 0.001, Epoch: 9/9, Batch Size: 16, train Loss: 1.2166 Acc: 0.7893\n",
      "LR: 0.001, Epoch: 9/9, Batch Size: 16, val Loss: 1.3918 Acc: 0.6879\n",
      "LR: 0.001, Epoch: 0/9, Batch Size: 32, train Loss: 5.0473 Acc: 0.0433\n",
      "LR: 0.001, Epoch: 0/9, Batch Size: 32, val Loss: 4.5493 Acc: 0.1285\n",
      "LR: 0.001, Epoch: 1/9, Batch Size: 32, train Loss: 4.1273 Acc: 0.2407\n",
      "LR: 0.001, Epoch: 1/9, Batch Size: 32, val Loss: 3.6862 Acc: 0.3345\n",
      "LR: 0.001, Epoch: 2/9, Batch Size: 32, train Loss: 3.3205 Acc: 0.4229\n",
      "LR: 0.001, Epoch: 2/9, Batch Size: 32, val Loss: 2.9996 Acc: 0.4377\n",
      "LR: 0.001, Epoch: 3/9, Batch Size: 32, train Loss: 2.7018 Acc: 0.5501\n",
      "LR: 0.001, Epoch: 3/9, Batch Size: 32, val Loss: 2.5183 Acc: 0.5329\n",
      "LR: 0.001, Epoch: 4/9, Batch Size: 32, train Loss: 2.2631 Acc: 0.6178\n",
      "LR: 0.001, Epoch: 4/9, Batch Size: 32, val Loss: 2.1720 Acc: 0.5741\n",
      "LR: 0.001, Epoch: 5/9, Batch Size: 32, train Loss: 1.9447 Acc: 0.6747\n",
      "LR: 0.001, Epoch: 5/9, Batch Size: 32, val Loss: 1.9344 Acc: 0.6142\n",
      "LR: 0.001, Epoch: 6/9, Batch Size: 32, train Loss: 1.7001 Acc: 0.7122\n",
      "LR: 0.001, Epoch: 6/9, Batch Size: 32, val Loss: 1.7415 Acc: 0.6411\n",
      "LR: 0.001, Epoch: 7/9, Batch Size: 32, train Loss: 1.5059 Acc: 0.7421\n",
      "LR: 0.001, Epoch: 7/9, Batch Size: 32, val Loss: 1.6115 Acc: 0.6542\n",
      "LR: 0.001, Epoch: 8/9, Batch Size: 32, train Loss: 1.3611 Acc: 0.7638\n",
      "LR: 0.001, Epoch: 8/9, Batch Size: 32, val Loss: 1.5200 Acc: 0.6643\n",
      "LR: 0.001, Epoch: 9/9, Batch Size: 32, train Loss: 1.2393 Acc: 0.7827\n",
      "LR: 0.001, Epoch: 9/9, Batch Size: 32, val Loss: 1.4083 Acc: 0.6824\n",
      "LR: 0.001, Epoch: 0/14, Batch Size: 16, train Loss: 5.0368 Acc: 0.0479\n",
      "LR: 0.001, Epoch: 0/14, Batch Size: 16, val Loss: 4.5183 Acc: 0.1449\n",
      "LR: 0.001, Epoch: 1/14, Batch Size: 16, train Loss: 4.1210 Acc: 0.2404\n",
      "LR: 0.001, Epoch: 1/14, Batch Size: 16, val Loss: 3.6540 Acc: 0.3294\n",
      "LR: 0.001, Epoch: 2/14, Batch Size: 16, train Loss: 3.3049 Acc: 0.4193\n",
      "LR: 0.001, Epoch: 2/14, Batch Size: 16, val Loss: 2.9652 Acc: 0.4621\n",
      "LR: 0.001, Epoch: 3/14, Batch Size: 16, train Loss: 2.6884 Acc: 0.5461\n",
      "LR: 0.001, Epoch: 3/14, Batch Size: 16, val Loss: 2.4820 Acc: 0.5278\n",
      "LR: 0.001, Epoch: 4/14, Batch Size: 16, train Loss: 2.2550 Acc: 0.6167\n",
      "LR: 0.001, Epoch: 4/14, Batch Size: 16, val Loss: 2.1521 Acc: 0.5842\n",
      "LR: 0.001, Epoch: 5/14, Batch Size: 16, train Loss: 1.9369 Acc: 0.6708\n",
      "LR: 0.001, Epoch: 5/14, Batch Size: 16, val Loss: 1.9175 Acc: 0.6049\n",
      "LR: 0.001, Epoch: 6/14, Batch Size: 16, train Loss: 1.6884 Acc: 0.7087\n",
      "LR: 0.001, Epoch: 6/14, Batch Size: 16, val Loss: 1.7180 Acc: 0.6403\n",
      "LR: 0.001, Epoch: 7/14, Batch Size: 16, train Loss: 1.5032 Acc: 0.7434\n",
      "LR: 0.001, Epoch: 7/14, Batch Size: 16, val Loss: 1.6006 Acc: 0.6634\n",
      "LR: 0.001, Epoch: 8/14, Batch Size: 16, train Loss: 1.3509 Acc: 0.7610\n",
      "LR: 0.001, Epoch: 8/14, Batch Size: 16, val Loss: 1.4716 Acc: 0.6744\n",
      "LR: 0.001, Epoch: 9/14, Batch Size: 16, train Loss: 1.2223 Acc: 0.7829\n",
      "LR: 0.001, Epoch: 9/14, Batch Size: 16, val Loss: 1.3969 Acc: 0.6773\n",
      "LR: 0.001, Epoch: 10/14, Batch Size: 16, train Loss: 1.1215 Acc: 0.8020\n",
      "LR: 0.001, Epoch: 10/14, Batch Size: 16, val Loss: 1.3233 Acc: 0.7077\n",
      "LR: 0.001, Epoch: 11/14, Batch Size: 16, train Loss: 1.0299 Acc: 0.8171\n",
      "LR: 0.001, Epoch: 11/14, Batch Size: 16, val Loss: 1.2675 Acc: 0.7056\n",
      "LR: 0.001, Epoch: 12/14, Batch Size: 16, train Loss: 0.9444 Acc: 0.8339\n",
      "LR: 0.001, Epoch: 12/14, Batch Size: 16, val Loss: 1.2087 Acc: 0.7136\n",
      "LR: 0.001, Epoch: 13/14, Batch Size: 16, train Loss: 0.8849 Acc: 0.8443\n",
      "LR: 0.001, Epoch: 13/14, Batch Size: 16, val Loss: 1.1631 Acc: 0.7233\n",
      "LR: 0.001, Epoch: 14/14, Batch Size: 16, train Loss: 0.8247 Acc: 0.8505\n",
      "LR: 0.001, Epoch: 14/14, Batch Size: 16, val Loss: 1.1355 Acc: 0.7241\n",
      "LR: 0.001, Epoch: 0/14, Batch Size: 32, train Loss: 5.0246 Acc: 0.0464\n",
      "LR: 0.001, Epoch: 0/14, Batch Size: 32, val Loss: 4.5110 Acc: 0.1285\n",
      "LR: 0.001, Epoch: 1/14, Batch Size: 32, train Loss: 4.1093 Acc: 0.2315\n",
      "LR: 0.001, Epoch: 1/14, Batch Size: 32, val Loss: 3.6519 Acc: 0.3113\n",
      "LR: 0.001, Epoch: 2/14, Batch Size: 32, train Loss: 3.3019 Acc: 0.4195\n",
      "LR: 0.001, Epoch: 2/14, Batch Size: 32, val Loss: 2.9643 Acc: 0.4524\n",
      "LR: 0.001, Epoch: 3/14, Batch Size: 32, train Loss: 2.6989 Acc: 0.5487\n",
      "LR: 0.001, Epoch: 3/14, Batch Size: 32, val Loss: 2.4763 Acc: 0.5371\n",
      "LR: 0.001, Epoch: 4/14, Batch Size: 32, train Loss: 2.2606 Acc: 0.6255\n",
      "LR: 0.001, Epoch: 4/14, Batch Size: 32, val Loss: 2.1527 Acc: 0.5830\n",
      "LR: 0.001, Epoch: 5/14, Batch Size: 32, train Loss: 1.9352 Acc: 0.6777\n",
      "LR: 0.001, Epoch: 5/14, Batch Size: 32, val Loss: 1.9124 Acc: 0.6188\n",
      "LR: 0.001, Epoch: 6/14, Batch Size: 32, train Loss: 1.6976 Acc: 0.7053\n",
      "LR: 0.001, Epoch: 6/14, Batch Size: 32, val Loss: 1.7173 Acc: 0.6432\n",
      "LR: 0.001, Epoch: 7/14, Batch Size: 32, train Loss: 1.5083 Acc: 0.7384\n",
      "LR: 0.001, Epoch: 7/14, Batch Size: 32, val Loss: 1.5917 Acc: 0.6588\n",
      "LR: 0.001, Epoch: 8/14, Batch Size: 32, train Loss: 1.3522 Acc: 0.7657\n",
      "LR: 0.001, Epoch: 8/14, Batch Size: 32, val Loss: 1.4968 Acc: 0.6769\n",
      "LR: 0.001, Epoch: 9/14, Batch Size: 32, train Loss: 1.2305 Acc: 0.7818\n",
      "LR: 0.001, Epoch: 9/14, Batch Size: 32, val Loss: 1.3975 Acc: 0.6841\n",
      "LR: 0.001, Epoch: 10/14, Batch Size: 32, train Loss: 1.1225 Acc: 0.8013\n",
      "LR: 0.001, Epoch: 10/14, Batch Size: 32, val Loss: 1.3202 Acc: 0.7030\n",
      "LR: 0.001, Epoch: 11/14, Batch Size: 32, train Loss: 1.0370 Acc: 0.8172\n",
      "LR: 0.001, Epoch: 11/14, Batch Size: 32, val Loss: 1.2610 Acc: 0.7106\n",
      "LR: 0.001, Epoch: 12/14, Batch Size: 32, train Loss: 0.9572 Acc: 0.8325\n",
      "LR: 0.001, Epoch: 12/14, Batch Size: 32, val Loss: 1.2263 Acc: 0.7110\n",
      "LR: 0.001, Epoch: 13/14, Batch Size: 32, train Loss: 0.8899 Acc: 0.8406\n",
      "LR: 0.001, Epoch: 13/14, Batch Size: 32, val Loss: 1.1861 Acc: 0.7216\n",
      "LR: 0.001, Epoch: 14/14, Batch Size: 32, train Loss: 0.8260 Acc: 0.8547\n",
      "LR: 0.001, Epoch: 14/14, Batch Size: 32, val Loss: 1.1372 Acc: 0.7254\n",
      "LR: 0.001, Epoch: 0/19, Batch Size: 16, train Loss: 5.0256 Acc: 0.0451\n",
      "LR: 0.001, Epoch: 0/19, Batch Size: 16, val Loss: 4.5080 Acc: 0.1226\n",
      "LR: 0.001, Epoch: 1/19, Batch Size: 16, train Loss: 4.0955 Acc: 0.2380\n",
      "LR: 0.001, Epoch: 1/19, Batch Size: 16, val Loss: 3.6357 Acc: 0.3201\n",
      "LR: 0.001, Epoch: 2/19, Batch Size: 16, train Loss: 3.2841 Acc: 0.4243\n",
      "LR: 0.001, Epoch: 2/19, Batch Size: 16, val Loss: 2.9669 Acc: 0.4553\n",
      "LR: 0.001, Epoch: 3/19, Batch Size: 16, train Loss: 2.6774 Acc: 0.5456\n",
      "LR: 0.001, Epoch: 3/19, Batch Size: 16, val Loss: 2.4915 Acc: 0.5278\n",
      "LR: 0.001, Epoch: 4/19, Batch Size: 16, train Loss: 2.2467 Acc: 0.6224\n",
      "LR: 0.001, Epoch: 4/19, Batch Size: 16, val Loss: 2.1558 Acc: 0.5783\n",
      "LR: 0.001, Epoch: 5/19, Batch Size: 16, train Loss: 1.9267 Acc: 0.6744\n",
      "LR: 0.001, Epoch: 5/19, Batch Size: 16, val Loss: 1.9403 Acc: 0.6137\n",
      "LR: 0.001, Epoch: 6/19, Batch Size: 16, train Loss: 1.6897 Acc: 0.7078\n",
      "LR: 0.001, Epoch: 6/19, Batch Size: 16, val Loss: 1.7571 Acc: 0.6352\n",
      "LR: 0.001, Epoch: 7/19, Batch Size: 16, train Loss: 1.5078 Acc: 0.7413\n",
      "LR: 0.001, Epoch: 7/19, Batch Size: 16, val Loss: 1.6109 Acc: 0.6634\n",
      "LR: 0.001, Epoch: 8/19, Batch Size: 16, train Loss: 1.3574 Acc: 0.7617\n",
      "LR: 0.001, Epoch: 8/19, Batch Size: 16, val Loss: 1.4856 Acc: 0.6757\n",
      "LR: 0.001, Epoch: 9/19, Batch Size: 16, train Loss: 1.2238 Acc: 0.7852\n",
      "LR: 0.001, Epoch: 9/19, Batch Size: 16, val Loss: 1.4024 Acc: 0.6870\n",
      "LR: 0.001, Epoch: 10/19, Batch Size: 16, train Loss: 1.1275 Acc: 0.8032\n",
      "LR: 0.001, Epoch: 10/19, Batch Size: 16, val Loss: 1.3555 Acc: 0.6929\n",
      "LR: 0.001, Epoch: 11/19, Batch Size: 16, train Loss: 1.0370 Acc: 0.8162\n",
      "LR: 0.001, Epoch: 11/19, Batch Size: 16, val Loss: 1.2755 Acc: 0.6992\n",
      "LR: 0.001, Epoch: 12/19, Batch Size: 16, train Loss: 0.9617 Acc: 0.8298\n",
      "LR: 0.001, Epoch: 12/19, Batch Size: 16, val Loss: 1.2287 Acc: 0.7022\n",
      "LR: 0.001, Epoch: 13/19, Batch Size: 16, train Loss: 0.8797 Acc: 0.8477\n",
      "LR: 0.001, Epoch: 13/19, Batch Size: 16, val Loss: 1.2067 Acc: 0.7085\n",
      "LR: 0.001, Epoch: 14/19, Batch Size: 16, train Loss: 0.8270 Acc: 0.8566\n",
      "LR: 0.001, Epoch: 14/19, Batch Size: 16, val Loss: 1.1537 Acc: 0.7136\n",
      "LR: 0.001, Epoch: 15/19, Batch Size: 16, train Loss: 0.7740 Acc: 0.8658\n",
      "LR: 0.001, Epoch: 15/19, Batch Size: 16, val Loss: 1.1252 Acc: 0.7174\n",
      "LR: 0.001, Epoch: 16/19, Batch Size: 16, train Loss: 0.7224 Acc: 0.8767\n",
      "LR: 0.001, Epoch: 16/19, Batch Size: 16, val Loss: 1.1050 Acc: 0.7211\n",
      "LR: 0.001, Epoch: 17/19, Batch Size: 16, train Loss: 0.6709 Acc: 0.8887\n",
      "LR: 0.001, Epoch: 17/19, Batch Size: 16, val Loss: 1.0865 Acc: 0.7233\n",
      "LR: 0.001, Epoch: 18/19, Batch Size: 16, train Loss: 0.6341 Acc: 0.8940\n",
      "LR: 0.001, Epoch: 18/19, Batch Size: 16, val Loss: 1.0538 Acc: 0.7275\n",
      "LR: 0.001, Epoch: 19/19, Batch Size: 16, train Loss: 0.5970 Acc: 0.9032\n",
      "LR: 0.001, Epoch: 19/19, Batch Size: 16, val Loss: 1.0391 Acc: 0.7338\n",
      "LR: 0.001, Epoch: 0/19, Batch Size: 32, train Loss: 5.0309 Acc: 0.0493\n",
      "LR: 0.001, Epoch: 0/19, Batch Size: 32, val Loss: 4.5128 Acc: 0.1243\n",
      "LR: 0.001, Epoch: 1/19, Batch Size: 32, train Loss: 4.1017 Acc: 0.2407\n",
      "LR: 0.001, Epoch: 1/19, Batch Size: 32, val Loss: 3.6401 Acc: 0.3016\n",
      "LR: 0.001, Epoch: 2/19, Batch Size: 32, train Loss: 3.3033 Acc: 0.4299\n",
      "LR: 0.001, Epoch: 2/19, Batch Size: 32, val Loss: 2.9583 Acc: 0.4486\n",
      "LR: 0.001, Epoch: 3/19, Batch Size: 32, train Loss: 2.6882 Acc: 0.5449\n",
      "LR: 0.001, Epoch: 3/19, Batch Size: 32, val Loss: 2.4798 Acc: 0.5383\n",
      "LR: 0.001, Epoch: 4/19, Batch Size: 32, train Loss: 2.2409 Acc: 0.6260\n",
      "LR: 0.001, Epoch: 4/19, Batch Size: 32, val Loss: 2.1473 Acc: 0.5969\n",
      "LR: 0.001, Epoch: 5/19, Batch Size: 32, train Loss: 1.9225 Acc: 0.6742\n",
      "LR: 0.001, Epoch: 5/19, Batch Size: 32, val Loss: 1.8877 Acc: 0.6179\n",
      "LR: 0.001, Epoch: 6/19, Batch Size: 32, train Loss: 1.6838 Acc: 0.7094\n",
      "LR: 0.001, Epoch: 6/19, Batch Size: 32, val Loss: 1.7235 Acc: 0.6445\n",
      "LR: 0.001, Epoch: 7/19, Batch Size: 32, train Loss: 1.5002 Acc: 0.7390\n",
      "LR: 0.001, Epoch: 7/19, Batch Size: 32, val Loss: 1.5874 Acc: 0.6643\n",
      "LR: 0.001, Epoch: 8/19, Batch Size: 32, train Loss: 1.3464 Acc: 0.7614\n",
      "LR: 0.001, Epoch: 8/19, Batch Size: 32, val Loss: 1.4920 Acc: 0.6811\n",
      "LR: 0.001, Epoch: 9/19, Batch Size: 32, train Loss: 1.2234 Acc: 0.7822\n",
      "LR: 0.001, Epoch: 9/19, Batch Size: 32, val Loss: 1.4113 Acc: 0.6773\n",
      "LR: 0.001, Epoch: 10/19, Batch Size: 32, train Loss: 1.1202 Acc: 0.8031\n",
      "LR: 0.001, Epoch: 10/19, Batch Size: 32, val Loss: 1.3382 Acc: 0.6908\n",
      "LR: 0.001, Epoch: 11/19, Batch Size: 32, train Loss: 1.0335 Acc: 0.8179\n",
      "LR: 0.001, Epoch: 11/19, Batch Size: 32, val Loss: 1.2814 Acc: 0.7056\n",
      "LR: 0.001, Epoch: 12/19, Batch Size: 32, train Loss: 0.9492 Acc: 0.8362\n",
      "LR: 0.001, Epoch: 12/19, Batch Size: 32, val Loss: 1.2307 Acc: 0.7098\n",
      "LR: 0.001, Epoch: 13/19, Batch Size: 32, train Loss: 0.8919 Acc: 0.8419\n",
      "LR: 0.001, Epoch: 13/19, Batch Size: 32, val Loss: 1.1835 Acc: 0.7152\n",
      "LR: 0.001, Epoch: 14/19, Batch Size: 32, train Loss: 0.8228 Acc: 0.8556\n",
      "LR: 0.001, Epoch: 14/19, Batch Size: 32, val Loss: 1.1454 Acc: 0.7228\n",
      "LR: 0.001, Epoch: 15/19, Batch Size: 32, train Loss: 0.7717 Acc: 0.8656\n",
      "LR: 0.001, Epoch: 15/19, Batch Size: 32, val Loss: 1.1197 Acc: 0.7254\n",
      "LR: 0.001, Epoch: 16/19, Batch Size: 32, train Loss: 0.7163 Acc: 0.8749\n",
      "LR: 0.001, Epoch: 16/19, Batch Size: 32, val Loss: 1.0894 Acc: 0.7287\n",
      "LR: 0.001, Epoch: 17/19, Batch Size: 32, train Loss: 0.6751 Acc: 0.8861\n",
      "LR: 0.001, Epoch: 17/19, Batch Size: 32, val Loss: 1.0738 Acc: 0.7300\n",
      "LR: 0.001, Epoch: 18/19, Batch Size: 32, train Loss: 0.6317 Acc: 0.8971\n",
      "LR: 0.001, Epoch: 18/19, Batch Size: 32, val Loss: 1.0511 Acc: 0.7346\n",
      "LR: 0.001, Epoch: 19/19, Batch Size: 32, train Loss: 0.5945 Acc: 0.9050\n",
      "LR: 0.001, Epoch: 19/19, Batch Size: 32, val Loss: 1.0325 Acc: 0.7380\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# 设置 device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练代码\n",
    "learning_rates = [1e-2, 1e-3]\n",
    "num_epochs_list = [10, 15, 20]\n",
    "batch_sizes = [16, 32]\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for num_epochs in num_epochs_list:\n",
    "        for batch_size in batch_sizes:\n",
    "            # 重新初始化数据加载器\n",
    "            dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "            \n",
    "            # 重新初始化模型和优化器\n",
    "            model = models.resnet18(pretrained=True)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, 200)\n",
    "            model = model.to(device)\n",
    "            other_params = [param for name, param in model.named_parameters() if \"fc\" not in name]\n",
    "            optimizer = optim.SGD([\n",
    "    {'params': model.fc.parameters(), 'lr': lr},\n",
    "    {'params': other_params, 'lr': lr * 0.1}\n",
    "], momentum=0.9, weight_decay=1e-4)\n",
    "            \n",
    "            # 开始训练\n",
    "            for epoch in range(num_epochs):\n",
    "                for phase in ['train', 'val']:\n",
    "                    if phase == 'train':\n",
    "                        model.train()\n",
    "                    else:\n",
    "                        model.eval()\n",
    "                    \n",
    "                    running_loss = 0.0\n",
    "                    running_corrects = 0\n",
    "                    \n",
    "                    for inputs, labels in dataloaders[phase]:\n",
    "                        inputs = inputs.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        \n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        with torch.set_grad_enabled(phase == 'train'):\n",
    "                            outputs = model(inputs)\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            \n",
    "                            if phase == 'train':\n",
    "                                loss.backward()\n",
    "                                optimizer.step()\n",
    "                        \n",
    "                        running_loss += loss.item() * inputs.size(0)\n",
    "                        running_corrects += torch.sum(preds == labels.data)\n",
    "                    \n",
    "                    epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "                    epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "                    \n",
    "                    print(f'LR: {lr}, Epoch: {epoch}/{num_epochs - 1}, Batch Size: {batch_size}, {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e06457",
   "metadata": {},
   "source": [
    " # 训练为预训练过的模型，训练50轮（轮数太少无法收敛，太多则会过拟合，所以选择50轮）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96062a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\yolov8\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\Anaconda\\envs\\yolov8\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch train Loss: 5.2785 Acc: 0.0092\n",
      "Scratch val Loss: 5.1492 Acc: 0.0135\n",
      "Scratch train Loss: 5.0180 Acc: 0.0241\n",
      "Scratch val Loss: 4.8945 Acc: 0.0312\n",
      "Scratch train Loss: 4.7761 Acc: 0.0359\n",
      "Scratch val Loss: 4.7120 Acc: 0.0447\n",
      "Scratch train Loss: 4.5866 Acc: 0.0468\n",
      "Scratch val Loss: 4.5505 Acc: 0.0518\n",
      "Scratch train Loss: 4.4235 Acc: 0.0637\n",
      "Scratch val Loss: 4.3879 Acc: 0.0611\n",
      "Scratch train Loss: 4.2778 Acc: 0.0818\n",
      "Scratch val Loss: 4.3064 Acc: 0.0775\n",
      "Scratch train Loss: 4.1449 Acc: 0.0963\n",
      "Scratch val Loss: 4.2145 Acc: 0.0906\n",
      "Scratch train Loss: 4.0139 Acc: 0.1167\n",
      "Scratch val Loss: 4.1565 Acc: 0.0906\n",
      "Scratch train Loss: 3.8899 Acc: 0.1313\n",
      "Scratch val Loss: 4.1716 Acc: 0.0952\n",
      "Scratch train Loss: 3.7733 Acc: 0.1503\n",
      "Scratch val Loss: 3.9828 Acc: 0.1163\n",
      "Scratch train Loss: 3.6546 Acc: 0.1656\n",
      "Scratch val Loss: 3.8321 Acc: 0.1276\n",
      "Scratch train Loss: 3.5340 Acc: 0.1839\n",
      "Scratch val Loss: 3.8713 Acc: 0.1306\n",
      "Scratch train Loss: 3.4159 Acc: 0.1977\n",
      "Scratch val Loss: 3.6679 Acc: 0.1702\n",
      "Scratch train Loss: 3.3070 Acc: 0.2216\n",
      "Scratch val Loss: 3.5937 Acc: 0.1723\n",
      "Scratch train Loss: 3.1849 Acc: 0.2395\n",
      "Scratch val Loss: 3.6157 Acc: 0.1740\n",
      "Scratch train Loss: 3.0633 Acc: 0.2644\n",
      "Scratch val Loss: 3.5761 Acc: 0.1858\n",
      "Scratch train Loss: 2.9559 Acc: 0.2738\n",
      "Scratch val Loss: 3.5128 Acc: 0.1858\n",
      "Scratch train Loss: 2.8518 Acc: 0.3031\n",
      "Scratch val Loss: 3.6014 Acc: 0.1778\n",
      "Scratch train Loss: 2.7322 Acc: 0.3248\n",
      "Scratch val Loss: 3.2874 Acc: 0.2203\n",
      "Scratch train Loss: 2.6324 Acc: 0.3448\n",
      "Scratch val Loss: 3.2345 Acc: 0.2266\n",
      "Scratch train Loss: 2.4978 Acc: 0.3794\n",
      "Scratch val Loss: 3.2766 Acc: 0.2106\n",
      "Scratch train Loss: 2.4031 Acc: 0.3972\n",
      "Scratch val Loss: 3.1891 Acc: 0.2397\n",
      "Scratch train Loss: 2.2942 Acc: 0.4213\n",
      "Scratch val Loss: 3.2138 Acc: 0.2414\n",
      "Scratch train Loss: 2.1737 Acc: 0.4594\n",
      "Scratch val Loss: 3.6699 Acc: 0.1752\n",
      "Scratch train Loss: 2.0411 Acc: 0.4845\n",
      "Scratch val Loss: 3.1461 Acc: 0.2359\n",
      "Scratch train Loss: 1.9236 Acc: 0.5142\n",
      "Scratch val Loss: 3.5312 Acc: 0.2216\n",
      "Scratch train Loss: 1.8007 Acc: 0.5462\n",
      "Scratch val Loss: 3.1082 Acc: 0.2553\n",
      "Scratch train Loss: 1.6547 Acc: 0.5843\n",
      "Scratch val Loss: 3.5504 Acc: 0.2270\n",
      "Scratch train Loss: 1.5269 Acc: 0.6235\n",
      "Scratch val Loss: 3.2258 Acc: 0.2439\n",
      "Scratch train Loss: 1.3929 Acc: 0.6651\n",
      "Scratch val Loss: 3.2457 Acc: 0.2730\n",
      "Scratch train Loss: 1.2377 Acc: 0.7054\n",
      "Scratch val Loss: 3.7317 Acc: 0.2376\n",
      "Scratch train Loss: 1.1007 Acc: 0.7512\n",
      "Scratch val Loss: 3.4773 Acc: 0.2544\n",
      "Scratch train Loss: 0.9599 Acc: 0.7963\n",
      "Scratch val Loss: 3.6798 Acc: 0.2401\n",
      "Scratch train Loss: 0.8493 Acc: 0.8321\n",
      "Scratch val Loss: 3.4750 Acc: 0.2553\n",
      "Scratch train Loss: 0.7400 Acc: 0.8600\n",
      "Scratch val Loss: 3.6094 Acc: 0.2607\n",
      "Scratch train Loss: 0.6354 Acc: 0.8912\n",
      "Scratch val Loss: 3.2616 Acc: 0.2953\n",
      "Scratch train Loss: 0.5530 Acc: 0.9162\n",
      "Scratch val Loss: 3.4801 Acc: 0.2730\n",
      "Scratch train Loss: 0.4363 Acc: 0.9468\n",
      "Scratch val Loss: 3.8635 Acc: 0.2481\n",
      "Scratch train Loss: 0.3616 Acc: 0.9591\n",
      "Scratch val Loss: 3.6181 Acc: 0.2329\n",
      "Scratch train Loss: 0.3114 Acc: 0.9706\n",
      "Scratch val Loss: 3.5082 Acc: 0.2591\n",
      "Scratch train Loss: 0.2589 Acc: 0.9834\n",
      "Scratch val Loss: 3.8862 Acc: 0.2498\n",
      "Scratch train Loss: 0.2094 Acc: 0.9890\n",
      "Scratch val Loss: 3.7253 Acc: 0.2363\n",
      "Scratch train Loss: 0.2184 Acc: 0.9848\n",
      "Scratch val Loss: 3.8095 Acc: 0.2418\n",
      "Scratch train Loss: 0.1821 Acc: 0.9904\n",
      "Scratch val Loss: 3.4908 Acc: 0.2751\n",
      "Scratch train Loss: 0.1463 Acc: 0.9931\n",
      "Scratch val Loss: 3.5706 Acc: 0.2805\n",
      "Scratch train Loss: 0.1187 Acc: 0.9969\n",
      "Scratch val Loss: 3.7319 Acc: 0.2620\n",
      "Scratch train Loss: 0.1104 Acc: 0.9967\n",
      "Scratch val Loss: 3.6690 Acc: 0.2650\n",
      "Scratch train Loss: 0.0943 Acc: 0.9977\n",
      "Scratch val Loss: 4.0772 Acc: 0.2506\n",
      "Scratch train Loss: 0.0973 Acc: 0.9964\n",
      "Scratch val Loss: 3.6144 Acc: 0.2801\n",
      "Scratch train Loss: 0.0811 Acc: 0.9987\n",
      "Scratch val Loss: 3.5801 Acc: 0.2612\n"
     ]
    }
   ],
   "source": [
    "# 初始化未预训练的ResNet-18模型\n",
    "model_scratch = models.resnet18(pretrained=False)\n",
    "num_ftrs_scratch = model_scratch.fc.in_features\n",
    "model_scratch.fc = nn.Linear(num_ftrs_scratch, 200)\n",
    "model_scratch = model_scratch.to(device)\n",
    "\n",
    "# 设置优化器\n",
    "optimizer_scratch = optim.SGD(model_scratch.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "# 设置损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model_scratch.train()\n",
    "        else:\n",
    "            model_scratch.eval()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer_scratch.zero_grad()\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model_scratch(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer_scratch.step()\n",
    "            \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        \n",
    "        epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "        epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "        \n",
    "        print(f'Scratch {phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e5e90",
   "metadata": {},
   "source": [
    "# 将训练结果使用Tensorboard展示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e6e139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# 读取数据\n",
    "train_file_path = 'D:/FDU/graduate/研一下/神网/期中作业/训练集.xlsx'\n",
    "validation_file_path = 'D:/FDU/graduate/研一下/神网/期中作业/验证集.xlsx'\n",
    "train_df = pd.read_excel(train_file_path)\n",
    "validation_df = pd.read_excel(validation_file_path)\n",
    "\n",
    "# 创建一个日志目录\n",
    "user_home = os.path.expanduser('~')\n",
    "log_dir = os.path.join(user_home, \"tensorboard_logs\", \"training_phases_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "os.makedirs(log_dir, exist_ok=True)  # 确保目录存在\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "# 分割训练阶段\n",
    "train_phase_num = 0\n",
    "validation_phase_num = 0\n",
    "\n",
    "with writer.as_default():\n",
    "    # 记录训练集数据\n",
    "    for i in range(len(train_df)):\n",
    "        epoch = train_df.loc[i, 'Epoch']\n",
    "        train_loss = train_df.loc[i, 'Train_Loss']\n",
    "        train_accuracy = train_df.loc[i, 'Acc']\n",
    "\n",
    "        # 检测到新的训练阶段\n",
    "        if epoch == 0 and i != 0:\n",
    "            train_phase_num += 1\n",
    "        \n",
    "        # 写入 TensorBoard 日志\n",
    "        tf.summary.scalar(f'Train_Phase_{train_phase_num}/loss', train_loss, step=epoch)\n",
    "        tf.summary.scalar(f'Train_Phase_{train_phase_num}/accuracy', train_accuracy, step=epoch)\n",
    "\n",
    "    # 记录验证集数据\n",
    "    for i in range(len(validation_df)):\n",
    "        epoch = validation_df.loc[i, 'Epoch']\n",
    "        validation_loss = validation_df.loc[i, 'Val_Loss']\n",
    "        validation_accuracy = validation_df.loc[i, 'Acc']\n",
    "\n",
    "        # 检测到新的验证阶段\n",
    "        if epoch == 0 and i != 0:\n",
    "            validation_phase_num += 1\n",
    "        \n",
    "        # 写入 TensorBoard 日志\n",
    "        tf.summary.scalar(f'Validation_Phase_{validation_phase_num}/loss', validation_loss, step=epoch)\n",
    "        tf.summary.scalar(f'Validation_Phase_{validation_phase_num}/accuracy', validation_accuracy, step=epoch)\n",
    "\n",
    "    writer.flush()\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "# 设置日志目录\n",
    "log_dir = os.path.expanduser('~/tensorboard_logs')\n",
    "\n",
    "# 启动 TensorBoard 的命令\n",
    "tensorboard_cmd = ['tensorboard', '--logdir', log_dir, '--host', 'localhost', '--port', '6006']\n",
    "\n",
    "# 使用 subprocess 启动 TensorBoard\n",
    "tensorboard_proc = subprocess.Popen(tensorboard_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "# 输出访问 URL\n",
    "print(\"TensorBoard 已启动，请访问 http://localhost:6006 查看可视化结果。\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 加载Excel文件\n",
    "train_df = pd.read_excel('D:/FDU/graduate/研一下/神网/期中作业/未提前训练的训练集.xlsx')\n",
    "val_df = pd.read_excel('D:/FDU/graduate/研一下/神网/期中作业/未提前训练的验证集.xlsx')\n",
    "\n",
    "# 日志目录存在于用户的主目录下\n",
    "home_dir = os.path.expanduser('~')\n",
    "log_dir = os.path.join(home_dir, 'tensorboard_logs')\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# 创建日志写入器\n",
    "train_log_dir = os.path.join(log_dir, 'train')\n",
    "val_log_dir = os.path.join(log_dir, 'val')\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)\n",
    "\n",
    "# 从数据框中获取值\n",
    "train_loss = train_df['Train_Loss'].values\n",
    "train_acc = train_df['Acc'].values\n",
    "val_loss = val_df['Val_Loss'].values\n",
    "val_acc = val_df['Acc'].values\n",
    "\n",
    "# 写入摘要\n",
    "with train_summary_writer.as_default():\n",
    "    for epoch, (loss, acc) in enumerate(zip(train_loss, train_acc)):\n",
    "        tf.summary.scalar('loss', loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', acc, step=epoch)\n",
    "\n",
    "with val_summary_writer.as_default():\n",
    "    for epoch, (loss, acc) in enumerate(zip(val_loss, val_acc)):\n",
    "        tf.summary.scalar('loss', loss, step=epoch)\n",
    "        tf.summary.scalar('accuracy', acc, step=epoch)\n",
    "\n",
    "print(f'TensorBoard日志已保存到目录 \"{log_dir}\"。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f08155a",
   "metadata": {},
   "source": [
    "# 使用最优超参数进行训练并与50轮进行对比分析\n",
    "# 最优超参数为：learning_rate = 0.01， num_epochs = 15， batch_size = 16\n",
    "# 最后将模型权重保存到指定文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# 设置随机种子以确保结果的可重复性\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# 数据路径\n",
    "data_dir = 'D:/FDU/graduate/研一下/神网/期中作业/CUB_200_2011/CUB_200_2011/images'\n",
    "\n",
    "# 定义数据转换\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# 加载整个数据集\n",
    "full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "\n",
    "# 获取类别索引\n",
    "class_indices = {cls: [] for cls in full_dataset.classes}\n",
    "\n",
    "# 按类别收集图像索引\n",
    "for idx, (path, class_idx) in enumerate(full_dataset.imgs):\n",
    "    class_indices[full_dataset.classes[class_idx]].append(idx)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "\n",
    "for cls, indices in class_indices.items():\n",
    "    random.shuffle(indices)\n",
    "    split = int(0.8 * len(indices))  # 80% 训练，20% 验证\n",
    "    train_indices.extend(indices[:split])\n",
    "    val_indices.extend(indices[split:])\n",
    "\n",
    "# 创建训练集和验证集\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "# 应用不同的transform\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "val_dataset.dataset.transform = data_transforms['val']\n",
    "\n",
    "# 创建DataLoader\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 输出一些信息以确认\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Training images: {len(train_dataset)}\")\n",
    "print(f\"Validation images: {len(val_dataset)}\")\n",
    "\n",
    "# 设置 device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 训练和验证函数\n",
    "def train_model(model, criterion, optimizer, dataloaders, device, num_epochs=15, save_path='best_model.pth'):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    writer = SummaryWriter()  # 初始化TensorBoard writer\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # 记录到TensorBoard\n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "            else:\n",
    "                writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n",
    "            \n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    torch.save(model.state_dict(), save_path)  # 保存最佳模型权重\n",
    "    writer.close()  # 关闭TensorBoard writer\n",
    "    return model\n",
    "\n",
    "# 最优超参数\n",
    "learning_rate = 0.01\n",
    "num_epochs = 15\n",
    "batch_size = 16\n",
    "save_path = 'best_model.pth'\n",
    "\n",
    "# 重新初始化数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "dataloaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "# 重新初始化模型和优化器\n",
    "model = models.resnet18(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 200)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params': model.fc.parameters(), 'lr': learning_rate},\n",
    "    {'params': [param for name, param in model.named_parameters() if \"fc\" not in name], 'lr': learning_rate * 0.1}\n",
    "], momentum=0.9, weight_decay=1e-4)\n",
    "\n",
    "print(f'Learning rate: {learning_rate}, Epochs: {num_epochs}, Batch size: {batch_size}')\n",
    "model = train_model(model, criterion, optimizer, dataloaders, device, num_epochs, save_path)\n",
    "\n",
    "print(f\"Training completed. Best model saved to {save_path}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b860c2",
   "metadata": {},
   "source": [
    "## 实验结果\n",
    "* 训练过程中在训练集和验证集上的loss曲线和验证集上的accuracy变化如下图所示，其中loss曲线图如下：奇数列为训练集；偶数列为验证集；且参数依次为：learning rate=0.01，epoch=10，batch size=16；learning rate=0.01，epoch=10，batch size=32；learning rate=0.01，epoch=15，batch size=16；learning rate=0.01，epoch=15，batch size=32；learning rate=0.01，epoch=20，batch size=16；learning rate=0.01，epoch=20，batch size=32；learning rate=0.001，epoch=10，batch size=16；learning rate=0.001，epoch=10，batch size=32；learning rate=0.001，epoch=15，batch size=16；learning rate=0.001，epoch=15，batch size=32；learning rate=0.001，epoch=20，batch size=16；learning rate=0.001，epoch=20，batch size=32。\n",
    "\n",
    "<img src=\"./Train_Phase_0_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_0_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_1_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_1_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_2_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_2_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_3_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_3_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_4_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_4_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_5_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_5_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_6_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_6_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_7_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_7_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_8_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_8_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_9_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_9_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_10_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_10_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "<img src=\"./Train_Phase_11_loss.svg\" alt=\"训练集loss曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_11_loss.svg\" alt=\"验证集loss曲线\" width=\"700\">\n",
    "\n",
    "* 经预训练的验证集Accuracy曲线图如下；参数分布同上。\n",
    "\n",
    "<img src=\"./Validation_Phase_0_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_1_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_2_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_3_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_4_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_5_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_6_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_7_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_8_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_9_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_10_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Validation_Phase_11_accuracy.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "\n",
    "* 选择最优参数即LR=0.01,Batch size=16，Epoch=15，进行预训练的模型loss曲线图和Acc曲线图如下：\n",
    "\n",
    "<img src=\"./Loss_train.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Loss_val.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Accuracy_train.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "<img src=\"./Accuracy_val.svg\" alt=\"验证集Acc曲线\" width=\"700\">\n",
    "\n",
    "* 下图为未预训练的模型的loss曲线图和Acc曲线图，可以看出未预训练的模型的准确度相比预训练的模型要低得多。\n",
    "\n",
    "<img src=\"./loss.svg\" alt=\"未预训练的模型的loss曲线\" width=\"700\">\n",
    "<img src=\"./accuracy.svg\" alt=\"未预训练的模型的Acc曲线\" width=\"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fc10f",
   "metadata": {},
   "source": [
    "# 通过上面分析可以得出以下结论：\n",
    "# 经过预训练的模型可以在训练数据上训练5轮后达到收敛效果，在验证集准确率可以达到0.78\n",
    "# 随机初始化的模型会在40轮之后达到收敛，收敛后在验证集的最高准确率仅有0.29左右\n",
    "# 可以看出是否经过预训练对最后的结果是非常巨大的"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
